<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on Minimal</title>
    <link>https://yjhmelody.github.io/tags/ml/</link>
    <description>Recent content in Ml on Minimal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>cn-zh</language>
    <lastBuildDate>Sun, 24 Sep 2017 19:29:28 +0000</lastBuildDate>
    
	<atom:link href="https://yjhmelody.github.io/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>人工智能发展报告</title>
      <link>https://yjhmelody.github.io/blog/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%91%E5%B1%95%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Sun, 24 Sep 2017 19:29:28 +0000</pubDate>
      
      <guid>https://yjhmelody.github.io/blog/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%91%E5%B1%95%E6%8A%A5%E5%91%8A/</guid>
      <description>人工智能的发展 数据挖掘课程的作业报告，放在这里分享一下，都是一些蠢话。 符号主义（逻辑主义） 推理期 二十世纪五十年代到七十年代初，人工智能的研究处于“推理期”，那时候人们普遍认为机器获得智能的方法是赋予机器逻辑推理的能力。该阶段比较有代表的工作比如在1955年12月，赫伯特·西蒙（Herbert Simon）和艾伦·纽厄尔（Allen Newell）开发出逻辑理论家，这是世界上第一个人工智能程序，有能力证明罗素和怀特海《数学原理》第二章52个定理中的38个定理， 甚至在后来证明了全部52个定理。这两位也因此在1975年获得了图灵奖。
知识期 随着后来的研究进展，人们意识到仅仅具有逻辑推理能力是远远无法实现人工智能的。从二十世纪七十年代中期开始，人工智能的研究进入知识期。专家系统被大量开发出来，E.A. Feigenbaum 作为“知识工程”之父在1994年获得图灵奖。不过后来人们又意识到专家系统的“知识工程瓶颈”，把人类总结的知识教授给计算机是相当困难的。
机器学习 八十年代左右，“从样例中学习”的一大主流还是符号主义的思想，其代表如决策树（Decision tree）和基于逻辑的学习。决策树以信息论为基础，而基于逻辑的学习是归纳逻辑程序设计，可看作是机器学习与逻辑程序设计的交叉。
神经网络（联结主义） 九十年代中期之前，“从样例中学习”的另一个主流是基于神经网络的联结主义。1986年，D.E. Rumelhart 等人重新发明了BP算法，产生了深远影响，如今的深度学习最基本的概念便是BP算法。不过联结主义产生的是“黑箱”模型，从知识获取角度看有明显的弱点。然而，由于BP算法，深度学习算法在实际中非常有用，在2006年开始第三次神经网络高潮以深度学习之名重新爆发而来，在2012年之后成为人工智能的主流算法。当然，在九十年代统计学习兴起时，而又因为当时的局限性，曾经落入低潮。
统计学习 九十年代中期，“统计学习”迅速兴起并成为主流，如今依然是主流的机器学习算法，典型代表是SVM。早在九十年代之前，统计学习的许多基础理论已经出现，但因为联结主义的神经网络在九十年代具有局限性而没落后，统计学习被人瞄准目光而迅速流行起来。
谈谈自己的理解 前面的一些概括是我认为比较重要的历史的整理。如今看来，人工智能的历史虽然不漫长，却可以说的上丰富与多变。现如今，从学术、商业、工业角度来审视的人工智能，占据主流的是传统机器学习跟深度学习，然后才是强化学习跟规则学习（个人看法）。当然，如今的机器学习算法或多或少都用上了概率统计的知识。
深度学习如今越来越火热，在我自己开始留意机器学习的内容开始，深度学习相关的文章跟新闻就狂轰滥炸地映入我的眼里，以至于我没法不正视它。后来我也简单的接触深度学习，才发现它确实不可思议。在训练深度学习时，它就是在特征空间里不断逼近然后拟合到数据特征的“万能函数”，怪不得说它是万能近似图灵机。把它应用到许多领域感觉也就不奇怪了，当然它不一定比传统的机器学习和其他人工智能算法要更有效。
不过可能是因为深度学习的万用性导致它的黑箱性，大部分人在使用它来解决问题的时候，没有获得很好的解释，无论是从深度学习结构模型的理论角度还是问题本身特性的角度。只是在设计网络架构时，粗略地分析问题的特性，然后改良别人成功的架构跟“合理”的解释来解决问题。在问题较满意的解决后，没有合理的可解释性或者干脆从他人理解的来解释。
当然，从实用性跟工作角度来说，我觉得这样没错。深度学习跟你的剪刀和锤子一样，只是解决问题跟生产的工具，并不需要在意内部机理。这样的比喻来解释可能非常不妥当，不过我想表达的是，如今主流的人工智能方法：深度学习，虽然在构造时有比较好的理论基础，但在优化模型，优化架构，并且在解释优化可能性与优化思路上，缺乏理论。（对深度学习接触才几周，造成这样可能有偏差的认知，如有错误，欢迎指出）
相比于深度学习，传统偏向于统计的机器学习，可解释性就比较强了，而且在很多时候，从各种角度上与深度学习比较，丝毫不逊色。而且现实的问题非常复杂，蕴含大量不确定跟随机的事件，而概率论与数理统计就是对现实世界建立这样的一些模型跟假设，这方面的理论也算比较完善了，所以传统机器学习更能在数学理论上解释一些模型的行为。
而最近神经网络之父 Geoffrey Hinton 也表明对BP非常怀疑，应该抛弃它。BP如今是深度学习最常用的算法了，如果丢弃它，深度学习大概会大变样吧。如此看来，深度学习没有那么“完美”，仍需要大量基础研究甚至真的可能在未来被更好的算法替代。
展望未来的人工智能 如果想到三十年以后，大街小巷到处有序而不拥挤地行驶着无人车，载着乘客去景点；天空上时不时飞过一架架无人机，拍摄城市风景与监控城市安全；新闻报道是程序根据许多视频、图片。文本而撰写的；家里布满了传感器跟智能家具，许多繁琐事情可以通过简单对话跟指令来处理。 想到如果真能如此，不由对如今的人工智能算法寄予厚望并抱有乐观的想法。
但在我看来，上面的美好描述可能还是过于乐观了。
之前曾在哪里看到一个观点，20年后人工智能将会代替80%的工作。我觉得这个也太乐观了。当今现实生活中，从事脑力活动的人已经多余从事体力活动的人了，未来这个趋势应该更加明显。而以目前人工智能的能力，很多脑力活动不能代替（或者说，实现这些能代替脑力活动的程序需要更多脑力的脑力活动）。若人工智能的算法更加成熟稳定，许多人应该会从事人工智能相关的工作，建设基础设施（其实现在就有很多人往机器学习这个方向转业），普及人工智能，而只有少部分人推动人工智能的发展。
其实我一直不敢想像未来，这对于我太难了。十年前我对未来的展望似乎跟如今的现实大相径庭了。自己觉得可能出现的东西往往没有到来，反而出现一些超出以前认知的意外事物。说不定，以后深度学习也不再是实现人工智能的主力了，出现一些特定算法可以实现以前难以实现的智能，但却无法较好完成如今研究的方向。最坏的情况就是深度学习仍然是主流，而其理论仍然不明朗，调参也没有完备的方法论，人工智能发展停滞了几十年，直到我们这一代人死去，这后面的事情我也不想展望了。</description>
    </item>
    
    <item>
      <title>暑假机器学习总结</title>
      <link>https://yjhmelody.github.io/blog/posts/%E6%9A%91%E5%81%87%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</link>
      <pubDate>Wed, 13 Sep 2017 20:49:32 +0000</pubDate>
      
      <guid>https://yjhmelody.github.io/blog/posts/%E6%9A%91%E5%81%87%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</guid>
      <description>机器学习学习总结 经过两个月的学习，从对机器学习一点点懵懂认知，到现在对机器学习的基础知识跟体系有一定的认知。如今学习暂告一段落，总结如今学习过的重点知识，可以起到很好的复习作业，也是对两个月以来的交代。以下，我将按照每周学习进度来总结回顾机器学习的知识。
第一周 我们先学习了python编程基础，之后的学习是基于python各种库来实验的。我之前已经比较熟悉python，所以很快就完成这段学习；之后接触numpy，这是C语言为python编写的底层矩阵库，我之前也接触过，但比较浅，不过numpy封装的很好，用起来门槛也很低，很快就上手了；numpy之后就是pandas,它是数据分析常用的库，基于numpy，非常全面，我之前也用过，但基本需要重新学习，pandas比较难用，尤其是IO部分，有细粒度的操作，文档看起来也比较麻烦，没有示例，所以学习的过程中，是遇到问题再去查找方法，后面的学习pandas其实用到的也比较少。
以上内容大概花了两天，算是对机器学习的预备知识的准备。当然期间也学习了简单的使用anaconda，jupyter等工具，不再一一总结了。
之后开始学习基本的图像知识：
 颜色直方图,它所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置。之后有几次作业中要编写颜色直方图的处理，一开始还是挺棘手的。不过跟后来的图像特征提取就是小巫见大巫了 HOG特征，主要思想：在一副图像中，局部目标的表象和形状（appearance and shape）能够被梯度或边缘的方向密度分布很好地描述。（本质：梯度的统计信息，而梯度主要存在于边缘的地方）。这也是之后的需要实现的任务，学习过程中，只有这个博客资料可以参考，其他资料要么可能是全英文晦涩难懂，要么还不如这个。这个博客写的太精炼了，初学的时候，实现起来非常困难，以至于有些同学对根据这个博客写出算法的可能性产生怀疑。我这周的作业也卡在这里了，学习机器学习的时候反而是因为图像处理知识不过关。后来经过讲解对这些特征有更新的认识了，不过可能实现起来对于现在我的依然有些困难。 LBP特征、Haar特征也是这周的基础知识，我稍微学习了下LBP，发现比HOG要好懂，Haar并没有怎么看，最后这2个内容没有出现在作业里，我对这些也只有粗浅的认识。LBP（Local Binary Pattern，局部二值模式）是一种用来描述图像局部纹理特征的算子；它具有旋转不变性和灰度不变性等显著的优点。而Haar特征值反映了图像的灰度变化情况。  图像的部分内容学习后，开始了最基础的机器学习的内容：
 Regression，先从最简单的线性回归开始，线性回归可能是踏入机器学习世界的第一步吧，它教你如何做最简单的预测和机器学习比较本质的思维。前几周学习的知识大多是看李宏毅的视频，前几周感觉还是不错的。从线性回归开始，学到了基本的梯度下降思想和度量性能的代价函数。
 Error，第二节就是深入理解各种模型评估的知识，讲授了误差,偏差,方差的区别与联系。
 Gradient Descent，最后是深入学习梯度下降，学习推导基本的梯度下降，然后提出随机梯度下降(SGD)，从大量数据中随机选择一定量数据来训练，提高学习效率。除此之外，讲解了梯度下降的问题：学习率的选择。然后基于此讲解了一个算法Adagrad来控制学习率。
 最后还有一些比较杂的知识，了解了K折交叉验证的思想与作用，把数据集分割为训练集，验证集，测试集的思想与作用。各种距离度量，可以衡量样本近似度。最后完成两个作业
  图像知识可能在第一周学习，有点不知其有何用的感觉，就算想在思想上重视它，但没有实际用起来，还是难以深刻理解它的重要性吧。谈点个人感受，我其实挺不擅长也不太喜欢处理图像的，大一自己有简单接触过图像处理（跟现在的学的不太一样，而是常规的图像处理，不跟特征，知识等内容关联），虽然不比这些难，但也很吃力。
第二周 第二周主要学习的是分类的基础算法，分别学习K近邻,决策树,逻辑斯蒂回归：
 KNN的思想就是把某个样本跟其他所有样本进行距离度量并总和，该样本离哪个类别&amp;rsquo;最近&amp;rsquo;，就标记为该类别。KNN是懒惰学习的典型算法，即到需要分类的时候才使用上训练集。在学习KNN的时候还了解到矢量化编程的重要性，减少不必要的python for 循环可以利于底层numpy优化为并行代码，在我这次实验里速度提升了近百倍。
 Logistic Regression 该算法跟线性回归（跟感知器也类似）基本类似，不过它加入了sigmoid函数来进行分类而不是回归。后来学习深度学习才知道这里有神经网络的最基本的思想，或者说可能是最简单的神经网络了。
 Decision Tree决策树可能是到这周为止最难的算法了，写起来会特别长。思想其实很简单，就是树的思想 + 人类决策过程。常用的决策树有ID3,C4.5,C5.0,CART等，不过我只编写了最简单的ID3，其他决策树进行了简单的了解。学习过程中认识到如果生成完整的决策树，那会变得非常耗时，后面学习到剪枝知识（我在作业里面编写预剪枝了，不过效果很差）。学习决策树里面知道了一些信息论的知识，如信息熵,信息增益,纯度等知识，决策的依据便是依据这些数值来找到最佳决策特征。
  最后根据学到的知识完成一个井字棋胜负预测，不过我的模型很一般。这周的知识量不是很大，更多侧重机器学习基础编程，但是感觉学到东西很多的一周。
第三周 这周学习的机器学习非常强大和实用，是现在也很常用的模型：
 SVM非常理论，学到这里，我感觉我的高数白学了。其实现在我也不是很了解SVM，只对基本概念有了解。基本思想应该是把低维空间的非线性问题映射到高维空间线性问题来解决。然后里面概念非常多：支持向量的概念，距离度量，核函数，核方法，对偶问题，KKT等。
 集成学习非常实用且广用。许多一般模型集成后都可以大幅度提升性能。这周接触了许多集成学习算法：
 bagging，非常简单的集成学习算法，我后来实现了bagging决策树，性能提升了许多。基本思想是自助采样一些样本后，分别训练n个模型，然后进行投票决策。这样可以大大减少过拟合而提升性能。
 Random Forest，bagging算法的变体，基于决策树实现的。它与bagging的区别在于特化了决策树，在节点决策时，加入属性扰动（即只从一部分特征里选择最优特征），而bagging只有样本扰动（随机采集样本）。它的性能一般来说比bagging要好，我猜大概是加入了新的扰动后，更能避免决策树容易过拟合的缺点吧。
 其他如 boosting的adboost和xgboost，进行了简单了解，xgboost在kaggle里面很热门，因为性能特别好。不过这几个算法难度更大，我了解的也比较少。adboost的基本思想是让之前训练错误的部分对应的权重变大，让模型认识到这点。
  最后是一点点图像特征的知识bag of words model，对此进行一些了解
  第四周 这周学习无监督学习，主要是聚类跟降维，不过主要是侧重分析并运用这些技术：</description>
    </item>
    
    <item>
      <title>机器学习里的一些小概念</title>
      <link>https://yjhmelody.github.io/blog/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%87%8C%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E6%A6%82%E5%BF%B5/</link>
      <pubDate>Thu, 10 Aug 2017 13:53:56 +0000</pubDate>
      
      <guid>https://yjhmelody.github.io/blog/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%87%8C%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E6%A6%82%E5%BF%B5/</guid>
      <description> 机器学习里的一些小概念 轮廓系数 轮廓系数（Silhouette Coefficient），是聚类效果好坏的一种评价方式。最早由 Peter J. Rousseeuw 在 1986 提出。 它结合内聚度和分离度两种因素。可以用来在相同原始数据的基础上用来评价不同算法、或者算法不同运行方式对聚类结果所产生的影响。
计算过程 假设我们已经通过一定算法，将待分类数据进行了聚类。 常用的比如使用K-means，将待分类数据分为了k个簇。 对于簇中的每个向量。分别计算它们的轮廓系数。 对于其中的一个点 i 来说：
 计算 a(i) = average(i向量到所有它属于的簇中其它点的距离) 计算 b(i) = min(i向量到所有其他簇的点的平均距离)  那么i向量轮廓系数就为： {% raw %} $$ S(i) = \frac{b(i) - a(i)}{max{a(i), b(i)}} $$ {% endraw %}
判断 可见轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。将所有点的轮廓系数求平均，就是该聚类结果总的轮廓系数。
 a(i) ：i向量到同一簇内其他点不相似程度的平均值 b(i) ：i向量到其他簇的平均不相似程度的最小值 S(i)接近1，则说明样本i聚类合理； S(i)接近-1，则说明样本i更应该分类到另外的簇； 若S(i)近似为0，则说明样本i在两个簇的边界上。  未完待续 </description>
    </item>
    
    <item>
      <title>k-means</title>
      <link>https://yjhmelody.github.io/blog/posts/k-means/</link>
      <pubDate>Tue, 08 Aug 2017 16:18:09 +0000</pubDate>
      
      <guid>https://yjhmelody.github.io/blog/posts/k-means/</guid>
      <description>公式排版目前没法解决唉，博客写个数学公式怎么这么揪心，好不容易解决了公式显示问题，但排版又很难控制
k-means 基本原理 这个星期学无监督学习(unsuperviser-learning), 最基本的是k-means算法. k-means属于原型聚类:假设聚类结构能通过一组原型刻画. 而聚类本身是根据数据相似度来划分的,即&amp;rdquo;距离&amp;rdquo;.我这里不展开讲距离计算, 姑且用欧几里德距离来理解，即平时最常见的公式. {% raw %}
我们先写出k-means的最小化平方误差: 给定样本集合 $D = {x_1, x_2, \dots, x_m}$,
k-means对聚类所得的簇划分 $C = {C_1, C_2, \dots, Ck}$ 的最小平方误差是 $E = \sum{i=1}^k \sum_{x \in C_i} ||x - \mu_i||_2^2 $,
其中 $\mu_i = \frac{1}{|Ci|} \sum{x \in C_i}x$是簇$ C_i $ 的均值向量 {% endraw %}
可以看出来, 求该式子的最优解并不容易, 因为需要考察每个样本可能的划分情况(NP难问题), 所以k-means 采用贪心策略, 通过迭代优化求近似解.
感觉学完以上内容, 脑子里好像有了这个算法的轮廓了, 但仔细一想, 还有很多地方需要斟酌. k-means是划分为k个类别, 那一开始怎么划分呢? 我们想到一开始需要选k个样本当作中心点来计算距离, 根据这k个位置就求得每个样本离哪个位置最接近. 这样,第一次聚类就完成了, 但是由于这次聚类造成每个类别的中心点发生了偏移,
我们需要重新计算这时候的中心点,而由于发生了偏移,我们又需要重新计算所有样本最接近的&amp;hellip; 如何反复,直到中心点(即均值向量)不变, 便完成了聚类.</description>
    </item>
    
  </channel>
</rss>