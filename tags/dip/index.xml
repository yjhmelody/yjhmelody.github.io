<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dip on The LIFE</title>
    <link>/tags/dip/</link>
    <description>Recent content in Dip on The LIFE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 13 Sep 2017 20:49:32 +0000</lastBuildDate>
    
	<atom:link href="/tags/dip/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>暑假机器学习总结</title>
      <link>/blog/posts/%E6%9A%91%E5%81%87%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</link>
      <pubDate>Wed, 13 Sep 2017 20:49:32 +0000</pubDate>
      
      <guid>/blog/posts/%E6%9A%91%E5%81%87%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</guid>
      <description>机器学习学习总结 经过两个月的学习，从对机器学习一点点懵懂认知，到现在对机器学习的基础知识跟体系有一定的认知。如今学习暂告一段落，总结如今学习过的重点知识，可以起到很好的复习作业，也是对两个月以来的交代。以下，我将按照每周学习进度来总结回顾机器学习的知识。
第一周 我们先学习了python编程基础，之后的学习是基于python各种库来实验的。我之前已经比较熟悉python，所以很快就完成这段学习；之后接触numpy，这是C语言为python编写的底层矩阵库，我之前也接触过，但比较浅，不过numpy封装的很好，用起来门槛也很低，很快就上手了；numpy之后就是pandas,它是数据分析常用的库，基于numpy，非常全面，我之前也用过，但基本需要重新学习，pandas比较难用，尤其是IO部分，有细粒度的操作，文档看起来也比较麻烦，没有示例，所以学习的过程中，是遇到问题再去查找方法，后面的学习pandas其实用到的也比较少。
以上内容大概花了两天，算是对机器学习的预备知识的准备。当然期间也学习了简单的使用anaconda，jupyter等工具，不再一一总结了。
之后开始学习基本的图像知识：
 颜色直方图,它所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置。之后有几次作业中要编写颜色直方图的处理，一开始还是挺棘手的。不过跟后来的图像特征提取就是小巫见大巫了 HOG特征，主要思想：在一副图像中，局部目标的表象和形状（appearance and shape）能够被梯度或边缘的方向密度分布很好地描述。（本质：梯度的统计信息，而梯度主要存在于边缘的地方）。这也是之后的需要实现的任务，学习过程中，只有这个博客资料可以参考，其他资料要么可能是全英文晦涩难懂，要么还不如这个。这个博客写的太精炼了，初学的时候，实现起来非常困难，以至于有些同学对根据这个博客写出算法的可能性产生怀疑。我这周的作业也卡在这里了，学习机器学习的时候反而是因为图像处理知识不过关。后来经过讲解对这些特征有更新的认识了，不过可能实现起来对于现在我的依然有些困难。 LBP特征、Haar特征也是这周的基础知识，我稍微学习了下LBP，发现比HOG要好懂，Haar并没有怎么看，最后这2个内容没有出现在作业里，我对这些也只有粗浅的认识。LBP（Local Binary Pattern，局部二值模式）是一种用来描述图像局部纹理特征的算子；它具有旋转不变性和灰度不变性等显著的优点。而Haar特征值反映了图像的灰度变化情况。  图像的部分内容学习后，开始了最基础的机器学习的内容：
 Regression，先从最简单的线性回归开始，线性回归可能是踏入机器学习世界的第一步吧，它教你如何做最简单的预测和机器学习比较本质的思维。前几周学习的知识大多是看李宏毅的视频，前几周感觉还是不错的。从线性回归开始，学到了基本的梯度下降思想和度量性能的代价函数。
 Error，第二节就是深入理解各种模型评估的知识，讲授了误差,偏差,方差的区别与联系。
 Gradient Descent，最后是深入学习梯度下降，学习推导基本的梯度下降，然后提出随机梯度下降(SGD)，从大量数据中随机选择一定量数据来训练，提高学习效率。除此之外，讲解了梯度下降的问题：学习率的选择。然后基于此讲解了一个算法Adagrad来控制学习率。
 最后还有一些比较杂的知识，了解了K折交叉验证的思想与作用，把数据集分割为训练集，验证集，测试集的思想与作用。各种距离度量，可以衡量样本近似度。最后完成两个作业
  图像知识可能在第一周学习，有点不知其有何用的感觉，就算想在思想上重视它，但没有实际用起来，还是难以深刻理解它的重要性吧。谈点个人感受，我其实挺不擅长也不太喜欢处理图像的，大一自己有简单接触过图像处理（跟现在的学的不太一样，而是常规的图像处理，不跟特征，知识等内容关联），虽然不比这些难，但也很吃力。
第二周 第二周主要学习的是分类的基础算法，分别学习K近邻,决策树,逻辑斯蒂回归：
 KNN的思想就是把某个样本跟其他所有样本进行距离度量并总和，该样本离哪个类别&amp;rsquo;最近&amp;rsquo;，就标记为该类别。KNN是懒惰学习的典型算法，即到需要分类的时候才使用上训练集。在学习KNN的时候还了解到矢量化编程的重要性，减少不必要的python for 循环可以利于底层numpy优化为并行代码，在我这次实验里速度提升了近百倍。
 Logistic Regression 该算法跟线性回归（跟感知器也类似）基本类似，不过它加入了sigmoid函数来进行分类而不是回归。后来学习深度学习才知道这里有神经网络的最基本的思想，或者说可能是最简单的神经网络了。
 Decision Tree决策树可能是到这周为止最难的算法了，写起来会特别长。思想其实很简单，就是树的思想 + 人类决策过程。常用的决策树有ID3,C4.5,C5.0,CART等，不过我只编写了最简单的ID3，其他决策树进行了简单的了解。学习过程中认识到如果生成完整的决策树，那会变得非常耗时，后面学习到剪枝知识（我在作业里面编写预剪枝了，不过效果很差）。学习决策树里面知道了一些信息论的知识，如信息熵,信息增益,纯度等知识，决策的依据便是依据这些数值来找到最佳决策特征。
  最后根据学到的知识完成一个井字棋胜负预测，不过我的模型很一般。这周的知识量不是很大，更多侧重机器学习基础编程，但是感觉学到东西很多的一周。
第三周 这周学习的机器学习非常强大和实用，是现在也很常用的模型：
 SVM非常理论，学到这里，我感觉我的高数白学了。其实现在我也不是很了解SVM，只对基本概念有了解。基本思想应该是把低维空间的非线性问题映射到高维空间线性问题来解决。然后里面概念非常多：支持向量的概念，距离度量，核函数，核方法，对偶问题，KKT等。
 集成学习非常实用且广用。许多一般模型集成后都可以大幅度提升性能。这周接触了许多集成学习算法：
 bagging，非常简单的集成学习算法，我后来实现了bagging决策树，性能提升了许多。基本思想是自助采样一些样本后，分别训练n个模型，然后进行投票决策。这样可以大大减少过拟合而提升性能。
 Random Forest，bagging算法的变体，基于决策树实现的。它与bagging的区别在于特化了决策树，在节点决策时，加入属性扰动（即只从一部分特征里选择最优特征），而bagging只有样本扰动（随机采集样本）。它的性能一般来说比bagging要好，我猜大概是加入了新的扰动后，更能避免决策树容易过拟合的缺点吧。
 其他如 boosting的adboost和xgboost，进行了简单了解，xgboost在kaggle里面很热门，因为性能特别好。不过这几个算法难度更大，我了解的也比较少。adboost的基本思想是让之前训练错误的部分对应的权重变大，让模型认识到这点。
  最后是一点点图像特征的知识bag of words model，对此进行一些了解
  第四周 这周学习无监督学习，主要是聚类跟降维，不过主要是侧重分析并运用这些技术：</description>
    </item>
    
  </channel>
</rss>