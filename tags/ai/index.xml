<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai on Minimal</title>
    <link>/tags/ai/</link>
    <description>Recent content in Ai on Minimal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 24 Sep 2017 19:29:28 +0000</lastBuildDate>
    
	<atom:link href="/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>人工智能发展报告</title>
      <link>/blog/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%91%E5%B1%95%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Sun, 24 Sep 2017 19:29:28 +0000</pubDate>
      
      <guid>/blog/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%91%E5%B1%95%E6%8A%A5%E5%91%8A/</guid>
      <description>人工智能的发展 数据挖掘课程的作业报告，放在这里分享一下，都是一些蠢话。 符号主义（逻辑主义） 推理期 二十世纪五十年代到七十年代初，人工智能的研究处于“推理期”，那时候人们普遍认为机器获得智能的方法是赋予机器逻辑推理的能力。该阶段比较有代表的工作比如在1955年12月，赫伯特·西蒙（Herbert Simon）和艾伦·纽厄尔（Allen Newell）开发出逻辑理论家，这是世界上第一个人工智能程序，有能力证明罗素和怀特海《数学原理》第二章52个定理中的38个定理， 甚至在后来证明了全部52个定理。这两位也因此在1975年获得了图灵奖。
知识期 随着后来的研究进展，人们意识到仅仅具有逻辑推理能力是远远无法实现人工智能的。从二十世纪七十年代中期开始，人工智能的研究进入知识期。专家系统被大量开发出来，E.A. Feigenbaum 作为“知识工程”之父在1994年获得图灵奖。不过后来人们又意识到专家系统的“知识工程瓶颈”，把人类总结的知识教授给计算机是相当困难的。
机器学习 八十年代左右，“从样例中学习”的一大主流还是符号主义的思想，其代表如决策树（Decision tree）和基于逻辑的学习。决策树以信息论为基础，而基于逻辑的学习是归纳逻辑程序设计，可看作是机器学习与逻辑程序设计的交叉。
神经网络（联结主义） 九十年代中期之前，“从样例中学习”的另一个主流是基于神经网络的联结主义。1986年，D.E. Rumelhart 等人重新发明了BP算法，产生了深远影响，如今的深度学习最基本的概念便是BP算法。不过联结主义产生的是“黑箱”模型，从知识获取角度看有明显的弱点。然而，由于BP算法，深度学习算法在实际中非常有用，在2006年开始第三次神经网络高潮以深度学习之名重新爆发而来，在2012年之后成为人工智能的主流算法。当然，在九十年代统计学习兴起时，而又因为当时的局限性，曾经落入低潮。
统计学习 九十年代中期，“统计学习”迅速兴起并成为主流，如今依然是主流的机器学习算法，典型代表是SVM。早在九十年代之前，统计学习的许多基础理论已经出现，但因为联结主义的神经网络在九十年代具有局限性而没落后，统计学习被人瞄准目光而迅速流行起来。
谈谈自己的理解 前面的一些概括是我认为比较重要的历史的整理。如今看来，人工智能的历史虽然不漫长，却可以说的上丰富与多变。现如今，从学术、商业、工业角度来审视的人工智能，占据主流的是传统机器学习跟深度学习，然后才是强化学习跟规则学习（个人看法）。当然，如今的机器学习算法或多或少都用上了概率统计的知识。
深度学习如今越来越火热，在我自己开始留意机器学习的内容开始，深度学习相关的文章跟新闻就狂轰滥炸地映入我的眼里，以至于我没法不正视它。后来我也简单的接触深度学习，才发现它确实不可思议。在训练深度学习时，它就是在特征空间里不断逼近然后拟合到数据特征的“万能函数”，怪不得说它是万能近似图灵机。把它应用到许多领域感觉也就不奇怪了，当然它不一定比传统的机器学习和其他人工智能算法要更有效。
不过可能是因为深度学习的万用性导致它的黑箱性，大部分人在使用它来解决问题的时候，没有获得很好的解释，无论是从深度学习结构模型的理论角度还是问题本身特性的角度。只是在设计网络架构时，粗略地分析问题的特性，然后改良别人成功的架构跟“合理”的解释来解决问题。在问题较满意的解决后，没有合理的可解释性或者干脆从他人理解的来解释。
当然，从实用性跟工作角度来说，我觉得这样没错。深度学习跟你的剪刀和锤子一样，只是解决问题跟生产的工具，并不需要在意内部机理。这样的比喻来解释可能非常不妥当，不过我想表达的是，如今主流的人工智能方法：深度学习，虽然在构造时有比较好的理论基础，但在优化模型，优化架构，并且在解释优化可能性与优化思路上，缺乏理论。（对深度学习接触才几周，造成这样可能有偏差的认知，如有错误，欢迎指出）
相比于深度学习，传统偏向于统计的机器学习，可解释性就比较强了，而且在很多时候，从各种角度上与深度学习比较，丝毫不逊色。而且现实的问题非常复杂，蕴含大量不确定跟随机的事件，而概率论与数理统计就是对现实世界建立这样的一些模型跟假设，这方面的理论也算比较完善了，所以传统机器学习更能在数学理论上解释一些模型的行为。
而最近神经网络之父 Geoffrey Hinton 也表明对BP非常怀疑，应该抛弃它。BP如今是深度学习最常用的算法了，如果丢弃它，深度学习大概会大变样吧。如此看来，深度学习没有那么“完美”，仍需要大量基础研究甚至真的可能在未来被更好的算法替代。
展望未来的人工智能 如果想到三十年以后，大街小巷到处有序而不拥挤地行驶着无人车，载着乘客去景点；天空上时不时飞过一架架无人机，拍摄城市风景与监控城市安全；新闻报道是程序根据许多视频、图片。文本而撰写的；家里布满了传感器跟智能家具，许多繁琐事情可以通过简单对话跟指令来处理。 想到如果真能如此，不由对如今的人工智能算法寄予厚望并抱有乐观的想法。
但在我看来，上面的美好描述可能还是过于乐观了。
之前曾在哪里看到一个观点，20年后人工智能将会代替80%的工作。我觉得这个也太乐观了。当今现实生活中，从事脑力活动的人已经多余从事体力活动的人了，未来这个趋势应该更加明显。而以目前人工智能的能力，很多脑力活动不能代替（或者说，实现这些能代替脑力活动的程序需要更多脑力的脑力活动）。若人工智能的算法更加成熟稳定，许多人应该会从事人工智能相关的工作，建设基础设施（其实现在就有很多人往机器学习这个方向转业），普及人工智能，而只有少部分人推动人工智能的发展。
其实我一直不敢想像未来，这对于我太难了。十年前我对未来的展望似乎跟如今的现实大相径庭了。自己觉得可能出现的东西往往没有到来，反而出现一些超出以前认知的意外事物。说不定，以后深度学习也不再是实现人工智能的主力了，出现一些特定算法可以实现以前难以实现的智能，但却无法较好完成如今研究的方向。最坏的情况就是深度学习仍然是主流，而其理论仍然不明朗，调参也没有完备的方法论，人工智能发展停滞了几十年，直到我们这一代人死去，这后面的事情我也不想展望了。</description>
    </item>
    
  </channel>
</rss>